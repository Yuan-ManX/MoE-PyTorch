# MoE PyTorch

<p align="center">
  <img src="MoE.png" alt="MoE" style="display:block; margin:auto; width:680px;" />
</p>

PyTorch implementation of Sparsely-Gated Mixture-of-Experts (MoE).

[MoE](https://arxiv.org/abs/1701.06538) - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.
